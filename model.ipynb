{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Disaster Tweets Kaggle Competition\n",
    "Notebook by Kea Kohv, 2023"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Kaggle competition \"Natural Language Processing with Disaster Tweets\" has a dataset on Twitter tweets that are categorized as being about a disaster or not being about a disaster. The aim of the competition is to build a machine learning model that predicts which Tweets are about real disasters and which ones arenâ€™t. In short, it is a binary text classification task."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The contents of this notebook are the following:\n",
    "1. Importing libraries\n",
    "2. Reading in the data\n",
    "3. Training set overview and quality assessment\n",
    "4. Test set overview and quality assessment\n",
    "5. Data clean-up\n",
    "6. Modelling: A simple SGDClassifier\n",
    "7. Modelling: Simple Neural Network with BERT encodings\n",
    "7. Modelling: fine-tuning DistilBERT\n",
    "8. Concusions and thoughts of what else could be done"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code was used with Python version 3.10.4\n",
    "\n",
    "# Import the necessary libraries\n",
    "\n",
    "import pandas as pd # for analyzing, cleaning, exploring, and manipulating data\n",
    "import numpy as np # for working with arrays\n",
    "from datasets import Dataset # for data pre-processing\n",
    "import pickle # to store objects into file\n",
    "\n",
    "# sklearn for building ML models, cross-validation, data splitting, data pre-processing\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# tensorflow for building ML models\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras import optimizers, losses, activations\n",
    "\n",
    "# transformers for DistilBERT model\n",
    "from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification\n",
    "\n",
    "# data clean-up methods for this specific task\n",
    "from clean_data import clean_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data\n",
    "df_train = pd.read_csv('data/train.csv', dtype={'id': np.int16, 'target': np.int8})\n",
    "df_test = pd.read_csv('data/test.csv', dtype={'id': np.int16})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7613, 5), (3263, 4))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display data shapes\n",
    "df_train.shape, df_test.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training set overview and quality assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A look at the first rows in the training set\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7613 entries, 0 to 7612\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        7613 non-null   int16 \n",
      " 1   keyword   7552 non-null   object\n",
      " 2   location  5080 non-null   object\n",
      " 3   text      7613 non-null   object\n",
      " 4   target    7613 non-null   int8  \n",
      "dtypes: int16(1), int8(1), object(3)\n",
      "memory usage: 200.9+ KB\n"
     ]
    }
   ],
   "source": [
    "# Display dataframe info, incl non-null value count and data types\n",
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overview of null values in the training set as a percentage\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "id           0.000000\n",
       "keyword      0.801261\n",
       "location    33.272035\n",
       "text         0.000000\n",
       "target       0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See the % of null values\n",
    "print(\"Overview of null values in the training set as a percentage\")\n",
    "df_train.isnull().sum() * 100 / len(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overview of unique values in the training set\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "id          7613\n",
       "keyword      221\n",
       "location    3341\n",
       "text        7503\n",
       "target         2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See the unique values count\n",
    "print(\"Overview of unique values in the training set\")\n",
    "df_train.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicated rows in the training set:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See if there are any duplicates\n",
    "print(\"Duplicated rows in the training set:\")\n",
    "df_train.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target class balance assessment with 0 (no disaster) and 1 (disaster) value counts:\n",
      "0    4342\n",
      "1    3271\n",
      "Name: target, dtype: int64\n",
      "\n",
      "Target class distribution in percentages:\n",
      "0    57.034021\n",
      "1    42.965979\n",
      "Name: target, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Take a look at target class balance\n",
    "print('Target class balance assessment with 0 (no disaster) and 1 (disaster) value counts:')\n",
    "print(df_train['target'].value_counts())\n",
    "print()\n",
    "print('Target class distribution in percentages:')\n",
    "print(df_train['target'].value_counts() *100 / len(df_train))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, there are more samples with class 0 (no disaster) than with class 1 (disaster). However, the dataset is not very imbalanced."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test set overview and quality assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash\n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
       "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
       "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display first rows of the testset\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3263 entries, 0 to 3262\n",
      "Data columns (total 4 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        3263 non-null   int16 \n",
      " 1   keyword   3237 non-null   object\n",
      " 2   location  2158 non-null   object\n",
      " 3   text      3263 non-null   object\n",
      "dtypes: int16(1), object(3)\n",
      "memory usage: 83.0+ KB\n"
     ]
    }
   ],
   "source": [
    "# Display testset info, incl non-null value count and data types\n",
    "df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overview of null values in the test set as a percentage\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "id           0.000000\n",
       "keyword      0.796813\n",
       "location    33.864542\n",
       "text         0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at the percentage of null values\n",
    "# As can be seen, the text field has no null values, so let's stick with that\n",
    "print(\"Overview of null values in the test set as a percentage\")\n",
    "df_test.isnull().sum() * 100 / len(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overview of unique values in the test set\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "id          3263\n",
       "keyword      221\n",
       "location    1602\n",
       "text        3243\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display unique values count\n",
    "print(\"Overview of unique values in the test set\")\n",
    "df_test.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicated rows in the training set:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See if there are any duplicates\n",
    "print(\"Duplicated rows in the training set:\")\n",
    "df_test.duplicated().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data clean-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data clean-up can take around 10 minutes. # Alternative is to load the already cleaned data from file, see below.\n",
    "df_train['text_cleaned'] = df_train['text'].apply(lambda s : clean_data(s))\n",
    "df_test['text_cleaned'] = df_test['text'].apply(lambda s : clean_data(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned dataframes to file\n",
    "df_train.to_csv('data/train_cleaned.csv',index=False)\n",
    "df_test.to_csv('data/test_cleaned.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cleaned dataframes from file\n",
    "df_train = pd.read_csv('data/train_cleaned.csv')\n",
    "df_test = pd.read_csv('data/test_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An example of cleaned text\n",
      "Original:  .@NorwayMFA #Bahrain police had previously died in a road accident they were not killed by explosion https://t.co/gFJfgTodad\n",
      "Cleaned:  NorwayMFA Bahrain police had previously died in a road accident they were not killed by explosion \n"
     ]
    }
   ],
   "source": [
    "# An example of data cleaning\n",
    "print('An example of cleaned text')\n",
    "print('Original: ', df_train['text'][100])\n",
    "print('Cleaned: ', df_train['text_cleaned'][100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop keyword, location and text columns\n",
    "df_train.drop(columns=['keyword','location','text'], inplace=True)\n",
    "df_test.drop(columns=['keyword','location','text'], inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGDClassifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's try a simple and fast SGDClassifier. It is a regularized linear model with stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter (CV score=0.730):\n",
      "{'clf__alpha': 0.001, 'clf__loss': 'log_loss', 'clf__penalty': 'none'}\n"
     ]
    }
   ],
   "source": [
    "# Create a pipeline\n",
    "sgd = Pipeline([('vect', CountVectorizer()), # tokenization\n",
    "                ('tfidf', TfidfTransformer()), # to tf-idf\n",
    "                ('clf', SGDClassifier(random_state=123)), # classifier\n",
    "               ])\n",
    "\n",
    "# Grid that will be used to find the best hyperparameters during cross-validation\n",
    "params = {\n",
    "    \"clf__loss\" : [\"hinge\", \"log_loss\", \"squared_hinge\", \"modified_huber\"],\n",
    "    \"clf__alpha\" : [0.0001, 0.001, 0.01, 0.1],\n",
    "    \"clf__penalty\" : [\"l2\", \"l1\", \"none\"],\n",
    "}\n",
    "\n",
    "# Cross-validation hyperparameter optimization\n",
    "search = GridSearchCV(sgd, params, n_jobs=2)\n",
    "search.fit(df_train['text_cleaned'], df_train['target'])\n",
    "\n",
    "# Print out the validation accuracy and hyperparameters of the best model\n",
    "# Note that on Kaggle, the assessment metric is fscore, not accuracy\n",
    "print(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\n",
    "print(search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test data and save the predictions to a submission file\n",
    "y_pred = search.best_estimator_.predict(df_test['text_cleaned'])\n",
    "\n",
    "submission = pd.DataFrame()\n",
    "submission['id'] = df_test['id']\n",
    "submission['target'] = y_pred\n",
    "\n",
    "submission.to_csv('submissions/sgd_optimized.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The submission got fscore 0.786 on Kaggle which is not bad considering how fast and easy the training of this model was."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A simple neural network with BERT encodings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's make a simple neural network with BERT encodings as inputs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT (Bidirectional Encoder Representations from Transformers) provides dense vector representations for natural language by using a deep, pre-trained neural network with the Transformer architecture. It was originally published by Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova: \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\", 2018."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT encoder uses L=12 hidden layers (i.e., Transformer blocks), a hidden size of H=768, and A=12 attention heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the training set into training and validation sets.\n",
    "# Ideally, crossvalidation is preferrable but because training NNs takes a lot of time, I will not be doing that.\n",
    "# Use stratification to ensure class labels are \n",
    "X_train, X_valid, y_train, y_valid = train_test_split(df_train['text_cleaned'], df_train['target'], stratify=df_train['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.data_structures has been moved to tensorflow.python.trackable.data_structures. The old module will be deleted in version 2.11.\n"
     ]
    }
   ],
   "source": [
    "bert_preprocess = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\")\n",
    "bert_encoder = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bert layers\n",
    "text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "preprocessed_text = bert_preprocess(text_input)\n",
    "outputs = bert_encoder(preprocessed_text)\n",
    "\n",
    "# Neural network layers\n",
    "l = tf.keras.layers.Dropout(0.1, name=\"dropout\")(outputs['pooled_output'])\n",
    "l = tf.keras.layers.Dense(1, activation='sigmoid', name=\"output\")(l)\n",
    "\n",
    "# Use inputs and outputs to construct a final model\n",
    "model = tf.keras.Model(inputs=[text_input], outputs = [l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " text (InputLayer)              [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " keras_layer (KerasLayer)       {'input_type_ids':   0           ['text[0][0]']                   \n",
      "                                (None, 128),                                                      \n",
      "                                 'input_word_ids':                                                \n",
      "                                (None, 128),                                                      \n",
      "                                 'input_mask': (Non                                               \n",
      "                                e, 128)}                                                          \n",
      "                                                                                                  \n",
      " keras_layer_1 (KerasLayer)     {'sequence_output':  109482241   ['keras_layer[0][0]',            \n",
      "                                 (None, 128, 768),                'keras_layer[0][1]',            \n",
      "                                 'pooled_output': (               'keras_layer[0][2]']            \n",
      "                                None, 768),                                                       \n",
      "                                 'default': (None,                                                \n",
      "                                768),                                                             \n",
      "                                 'encoder_outputs':                                               \n",
      "                                 [(None, 128, 768),                                               \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768)]}                                               \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 768)          0           ['keras_layer_1[0][13]']         \n",
      "                                                                                                  \n",
      " output (Dense)                 (None, 1)            769         ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 109,483,010\n",
      "Trainable params: 769\n",
      "Non-trainable params: 109,482,241\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# See a summary of the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "357/357 [==============================] - 2280s 6s/step - loss: 0.6151 - accuracy: 0.6647\n",
      "Epoch 2/2\n",
      "357/357 [==============================] - 2022s 6s/step - loss: 0.5560 - accuracy: 0.7299\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23d2b8d7d90>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model on the training data\n",
    "# Ideally the number of epochs should be larger, currently there could be underfitting\n",
    "model.fit(X_train, y_train, epochs=2, batch_size = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60/60 [==============================] - 837s 14s/step - loss: 0.5438 - accuracy: 0.7463\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5437530875205994, 0.7463235259056091]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the accuracy on validation set\n",
    "# Not that this is ordinary accuracy, not f1score as on Kaggle\n",
    "model.evaluate(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102/102 [==============================] - 1651s 16s/step\n"
     ]
    }
   ],
   "source": [
    "# Get the testset label probabilities\n",
    "y_predicted = model.predict(df_test['text_cleaned'])\n",
    "y_predicted = y_predicted.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probabilities into predictions\n",
    "y_predicted = np.where(y_predicted > 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write predictions into a submission file\n",
    "submission = pd.DataFrame()\n",
    "submission['id'] = df_test['id']\n",
    "submission['target'] = y_predicted\n",
    "\n",
    "submission.to_csv('submissions/bert_simple_nn.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This simple NN got fscore 0.729 on Kaggle which is less than SGDClassifier but the training of the NN took a lot longer to train than the SGDClassifier. However, the NN is very simple, the hyperparameters are not optimized and the model is probably underfitting due to having run only 2 epochs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuned DistilBERT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, instead of creating a NN from scratch, let's use a pre-trained model and fine-tune it."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DistilBERT is a small, fast, cheap and light HuggingFace Transformer model trained by distilling BERT base.\n",
    "The following code is based on tutorials provided by HuggingFace https://huggingface.co/docs/transformers/model_doc/distilbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_layer_norm', 'activation_13', 'vocab_projector', 'vocab_transform']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['dropout_19', 'pre_classifier', 'classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Download the pre-trained model checkpoints\n",
    "distilbert_model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to help tokenize the text, used with the map function\n",
    "def tokenize(df):\n",
    "    return tokenizer(df[\"text_cleaned\"], padding=\"max_length\", truncation=True, max_length=140)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the training set into training and validation set\n",
    "train_set, valid_set = train_test_split(df_train, test_size=0.2, stratify=df_train['target']) # Also shuffles because default is shuffle=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23d4db9b4bd940b3a56349048a5a8d4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6090 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a045a69f353405794913537600cbb12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1523 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 16 samples used to estimate the error gradient\n",
    "# Potentially could also try higher batch size values. However, a too large value can lead to lower accuracy\n",
    "batch_size = 16\n",
    "\n",
    "# Tokenize the training set and and turn it into a tensorflow dataset\n",
    "dataset_train = Dataset.from_pandas(train_set[['text_cleaned','target']])\n",
    "train_tokenized = dataset_train.map(tokenize)\n",
    "tf_train = train_tokenized.to_tf_dataset(batch_size=batch_size, columns=['input_ids', 'attention_mask'], label_cols=['target'])\n",
    "\n",
    "# Tokenize the validation set and and turn it into a tensorflow dataset\n",
    "dataset_valid = Dataset.from_pandas(valid_set[['text_cleaned','target']])\n",
    "valid_tokenized = dataset_valid.map(tokenize)\n",
    "tf_valid = valid_tokenized.to_tf_dataset(batch_size=batch_size, columns=['input_ids', 'attention_mask'], label_cols=['target'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model with standard hyperparameters, not optimized.\n",
    "optimizer = optimizers.Adam(learning_rate=3e-5)\n",
    "loss = losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "distilbert_model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_distil_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " distilbert (TFDistilBertMai  multiple                 66362880  \n",
      " nLayer)                                                         \n",
      "                                                                 \n",
      " pre_classifier (Dense)      multiple                  590592    \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  1538      \n",
      "                                                                 \n",
      " dropout_19 (Dropout)        multiple                  0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 66,955,010\n",
      "Trainable params: 66,955,010\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "distilbert_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "381/381 [==============================] - 4470s 12s/step - loss: 0.4368 - accuracy: 0.8100\n",
      "Epoch 2/2\n",
      "381/381 [==============================] - 4783s 13s/step - loss: 0.3113 - accuracy: 0.8768\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2f583c9f880>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model on training data\n",
    "distilbert_model.fit(tf_train, batch_size=batch_size, epochs=2) # Training with 2 epochs took around 2 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 310s 3s/step - loss: 0.5096 - accuracy: 0.8162\n",
      "{'loss': 0.5095885992050171, 'accuracy': 0.8161523342132568}\n"
     ]
    }
   ],
   "source": [
    "# Get the validation accuracy\n",
    "benchmarks = distilbert_model.evaluate(tf_valid, return_dict=True, batch_size=batch_size)\n",
    "print(benchmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c337ddea9cf84b8680d93ab48ea1c566",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3263 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenize the test set and turn it into tensorflow dataset\n",
    "dataset_test = Dataset.from_pandas(df_test['text_cleaned'].to_frame())\n",
    "test_encoded = dataset_test.map(tokenize)\n",
    "tf_test = test_encoded.to_tf_dataset(batch_size=batch_size, columns=['input_ids', 'attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204/204 [==============================] - 771s 4s/step\n"
     ]
    }
   ],
   "source": [
    "# Get testset label probabilities\n",
    "y_pred = distilbert_model.predict(tf_test).logits\n",
    "y_pred = activations.softmax(tf.convert_to_tensor(y_pred)).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn the probabilities into labels\n",
    "y_pred_lbls = np.where(y_pred[:,:1] > 0.5, 0, 1)\n",
    "y_pred_lbls = y_pred_lbls.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write predictions into a submission file\n",
    "submission = pd.DataFrame()\n",
    "submission['id'] = df_test['id']\n",
    "submission['target'] = y_pred_lbls\n",
    "\n",
    "submission.to_csv('submissions/fine-tuned-distilbert.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This simple fine-tuned DistilBERT model got fscore 0.81 on Kaggle testset. For comparison, the best real submissions on Kaggle get around 0.84 so this model is close to that. However, if the hyperparameters were optimized and with more experimentaton, the result could be made even better."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What else could be done?\n",
    "\n",
    "A few ideas:\n",
    "- Further explorative data analysis, adding visualizations, displaying most often occurring words, phrases\n",
    "- Add metafeatures (e.g. word count, mean word length) to the dataset and use these for model training\n",
    "- Further data clean-up, e.g. correcting mislabeled samples\n",
    "- Increasing the simple NN model epochs to avoid underfitting, grid-search to optimize hyperparameters\n",
    "- Explore validation set predicted targets vs real targets with Confusion Matrix, assess precision and recall separately, report fscore as a metric\n",
    "- Try other models and approaches"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atm_2022",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
